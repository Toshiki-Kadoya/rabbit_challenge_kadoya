【要点のまとめ】
★第3章：情報理論

①[全体概要]
・情報量を数値化する方法を学習
　→自己情報量
　→シャノンエントロピ(自己情報量の期待値)
　→カルバック・ライブラー　ダイバージェンス(情報量の差)
　→交差エントロピー


②[講義内容の要約]
■統計学2-12
・情報科学
　情報をどうやって数量化するか？
　例えば、増えた量の表し方
　　→増加の比率を取る　Δw/w　⇒「情報の分かりやすさ」につながる。(確かに、変化に対して感度が高いパラメータだと、判断が容易。)
　　　　　　　　　　　　　　　 ⇒情報の増え方を示す関数を定義できないか(例えば、Δw/wを積分)。


■統計学2-13
・自己情報量
　I(x) = -log(P(x)) = log(W(x))
　　　↑対数の底が2のとき、単位はビット(bit)
　　　↑対数の底がネイピアのeのとき、単位は(nat)


■統計学2-14
・シャノンエントロピ(自己情報量の期待値)
　H(x) = E(I(x))
       = -E(log(P(x)))
       = -Σ(P(x)log(P(x)))

　→シャノンエントロピが最大になるものが、現実になるのではないかという考え方。


■統計学2-15
・カルバック・ライブラー　ダイバージェンス
　Dkl(P||Q) = ΣP(x)log[P(x)/Q(x)]
　↑同じ事象・確率変数における異なる確率分布P,Qの違いを表す。
　　例えば、
　　　→もともと想定されていた分布：Q
　　　→実際に測った分布：P

　→どのくらい情報量に差があるのか・情報利得を示す。


■統計学2-16
・交差エントロピー
　H(P,Q) = H(P) + Dkl(P||Q)
         = -ΣP(x)logQ(x)
    　↑KLダイバージェンスの一部分を取り出したもの。
　    ↑Qについての自己情報量をPの分布で平均している。


