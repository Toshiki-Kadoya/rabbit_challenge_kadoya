機械学習

【要点のまとめ】
①線形回帰モデル
・線形とは　→比例関係(例:y=ax+b[2次元]、z=ax+by+c[3次元]、・・・、n次元空間における超平面の方程式)
・∑ax=at・x (at:行列aの転置)：シグマ表記は行列表記で書き換えることができる。
・回帰問題：ある入力(説明変数=特徴量)から出力(目的変数)を予測する問題
・線形回帰モデル(1次元の場合：単回帰モデル＝直線)
　→教師あり学習。入力とm次元パラメータの線形結合を出力するモデル。
　　　　　　　　　→y=w0 + wt・x　(wt:行列wの転置、x:説明変数、y:予測値)
　　　　　　　　　　入力とパラメータの内積(線形結合)で表現。
・パラメータ(重み)を決めるための方法：最小二乗法(平均二乗誤差:MSEを最小とするパラメータを探索)など
　　　参考：二乗損失(二乗誤差)は、一般に外れ値に弱い→外れ値に引っ張られてしまうから。→二乗損失を用いる場合は、外れ値がないかどうか事前にチェックすべき。
　　　　　　外れ値に強い方法→絶対値損失、Huber損失、Tukey損失
・線形モデル数式の構築時、説明変数の数が不十分で誤差が大きな場合→　他に有意な説明変数があるのではないかと疑うべき。
　　　　　　　　　　　　　　　　　　　　　　　　(誤差項に、未検討の説明変数分の齟齬が加わる。)
・説明変数が多次元の場合→線形重回帰モデル(＝曲面)


②非線形回帰モデル
・非線形な回帰とは
　　→直線以外　例)2次元:x**2、3次元:x**3、sinx、cosx、log(x)などの非線形項・基底関数:φ(x)が含まれているもの
　　　※xからφ(x)に代えても、wについては線形のまま(linear-in-parameter)
・非線形回帰の基底関数の例: 多項式、ガウス型基底関数
・基底関数＝多項式の場合、次元を増やした方が表現力が増してよりフィッティングするのでは？
　　→次元を増やしても、フィッティング精度(誤差)に変化がなくなってくる
　　　→表現力の低いモデル:未学習、表現力の高いモデル:過学習
　　　　→適切な表現力のモデル(汎化性能が高いモデル)が望ましい
・過学習を抑制する方法
　1.学習データを増やす
　2.不要な基底関数(変数)を削除して表現力を抑止
　3.正則化を利用して表現力を抑止
　　→Ridge推定量[L2ノルムを利用]:パラメータを0に近づけるよう推定
　　　Lasso推定量[L1ノルムを利用]:いくつかのパラメータを正確に0に推定
・ホールドアウト法→データを学習用と評価用に分割
・交差検証　　　　→データを学習用と評価用に分割＋それを複数パターンで繰り返す
・グリッドサーチ　→全てのチューニングパラメータの組み合わせを試行し、評価
　　　　　　　　　　→最も良い評価値を持つチューニングパラメータを持つ組み合わせを採用


③ロジスティック回帰モデル
・分類問題→ある入力からクラス(例:0 or 1)に分類する問題
　　　　　　　　　　　　　→出力が連続した数値ではなくクラス・カテゴリになる。
・シグモイド関数:σ(x) = 1/1+exp(-x)
　　→入力は実数、出力は必ず0～1の値になる。
　　→特定のクラスに分類される確率を表現。
・シグモイド関数の微分:dσ(x)/dx = σ(x)(1 - σ(x))　※シグモイド関数の微分は、シグモイド関数で表わせる。
・データが与えられたときにY=1となる確率:P(Y=1 | x) = σ(w0 + w1x1 + ・・・+wmxm)
・尤度関数
　　→データは固定し、パラメータを変化させる。
　　→尤度関数を最大化するようなパラメータを選ぶ推定方法を最尤推定という。
　　　ここで、・データ[既知]⇒予測に使う入力データ(実データ)
　　　　　　　・パラメータ[未知]⇒そのデータを生成したであろう尤もらしい分布
・勾配降下法
　　→反復学習により、パラメータを逐次的に更新するアプローチ。
　　　　→尤度関数をパラメータで微分して0になる値を求める必要があるのだが、解析的にこの値を求めることが困難であるため、上記アプローチをとる。
・確率的勾配降下法
　　→データを一つずつランダムに選んでパラメータを更新する。
　　　勾配降下法でパラメータを1回更新するのと同じ計算量でパラメータをn回更新できるので、効率よく最適な解を探索可能。


④主成分分析
・多変量データの持つ構造をより少数個の指標に圧縮。(高次元⇒低次元)
　　→ただし、変量の個数を減らすことに伴う、情報の損失はなるべく小さくしたい。
　　→少数変数を利用した分析や可視化(2・3次元の場合)が実現可能。
・データの分散が最大になるように次元削減を行う。
　　→情報量を分散の大きさと捉える。
・寄与率:第k主成分の分散の全分散に対する割合(第k主成分が持つ情報量の割合)を算出できる。
　→累積寄与率：第1-k主成分まで圧縮した際の情報損失量の割合も算出可能。


⑤アルゴリズム
・k近傍法(kNN)
　→教師あり学習。
    最近傍のk個のデータを取ってきて、そのうち最も多く所属するクラスに分類する。(多数決)
  ・kを変化させると結果も変わることがある。
　・kを大きくすると決定境界が滑らかになる。
　　→kの選び方に注意。

・k-means
　→教師なし学習。クラスタリングの手法。与えられたデータをk個のクラスタに分類する。
　・k-meansのアルゴリズム
　　1.各クラスタ中心の初期値を設定する
　　2.各データ点に対して、各クラスタ中心との距離を計算し、最も距離が近いクラスタを割り当てる
　　3.クラスタの平均ベクトル（中心）を計算する
　　4.収束するまで2, 3の処理を繰り返す
　・欠点:初期値が近いとうまくクラスタリングできない。
　・kの値を変えるとクラスタリング結果も変化。
　　→kの選び方に注意。


⑥サポートベクターマシーン
・2クラス分類のための機械学習手法
・線形モデルの正負で2値分類を行う。
　→マージンが最大となる線形判別関数(≒決定境界)を求める。
　　　※マージン:形判別関数ともっとも近いデータ点との距離

・ソフトマージンSVM
　→サンプルを線形分離できないとき、誤差を許容し、誤差に対してペナルティを与える。
　　→線形分離できない場合でも対応。
　・パラメータCの大小で決定境界が変化。

・非線形分離
　→線形分離できないとき、徴空間に写像し、その空間で線形に分離する。

