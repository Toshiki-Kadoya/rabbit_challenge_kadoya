深層学習_後編_day3

【自己学習】

①Section1:再帰型ニューラルネットワークの概念
・RNNの応用例
　→機械翻訳、音声認識、画像の概要生成(画像の説明文を自動で作成)、説明文からの画像生成、
・RNNの種類
　→Simple RNN、LSTM、GRU、Bi-directional RNN、Attention RNN、Quasi-Recurrent Neural Network
・再帰構造　→処理内容の記述の中に、自身の呼び出しを行なうコードが含まれること。
・バイナリ加算　→2進数で足し算を行うこと。桁の繰り上がり処理は、過去から未来へ情報をつなげていくことに類似。
・構文木　→構文解析の経過や結果（またはそれら両方）を木構造で表したもの。
・BPTT　→誤差は最後の時刻であるTから最初の0に向かって伝播。
　　　　　ある時刻tにおけるoutput layerの誤差は「時刻tにおけるteacher(教師データ)とoutput(出力)の差異」と
　　　　　「t+1から伝播してきた誤差」の和になる。
　　　　　BPTTは最後のTまでのデータ、つまりすべての時系列データがなければ学習を行うことができない。
　　　　　BPTTには様々な課題があり、よってそれに対応するための学習方法もいろいろ考案されている。
　　　　　

②Section2:LSTM
・LSTM(Long short-term memory)　→従来のRNNでは学習できなかった長期依存(long-term dependencies)を学習可能である。
・RNNの勾配消失　→層が深いほど勾配がなくなっていく。RNNでは時系列が長い(長期の予測)≒層が深いことになるため、勾配消失が起こりやすい。
・勾配爆発　→勾配が、層を逆伝播するごとに指数関数的に大きくなっていくこと。(勾配消失の逆→活性化関数として恒等関数を使う場合や学習率が適切でない場合に起こる。)
・勾配のクリッピング　→勾配のノルムに対して一定の制約値(hard constraint)を設け、ミニバッチの学習毎に大きくなりすぎた勾配のノルムを補正するという方法。


③Section3:GRU
・GRU(Gated Recurrent Unit)　→LSTMと同様の性能を持つとされており、LSTMより計算量が少なく、高速に学習を進めることができる。
　　　　　　　　　　　　　　　 状態ベクトル数を減らす、ゲートコントローラ数を減らすという主に2つのアプローチで改善している。
・リセットゲート　→過去の情報をどれだけ忘れるかを決定する。
・更新ゲート　→過去の情報のどれだけを将来に渡す必要があるかを判断する。


④Section4:双方向RNN
・双方向RNN(Bidirectional RNN)　→RNNでは、ある状態の中間層の出力値を次の状態に順伝播するネットワークであったが、
　　　　　　　　　　　　　　　　　これに対し、中間層の出力を、未来への順伝播と過去への逆伝播の両方向に伝播するネットワークのこと。
・学習時に、過去と未来の情報の入力を必要とすることから、運用時も過去から未来までのすべての情報を入力してはじめて予測できるようになる。
　→そのため、応用範囲は限定される。例えば、過去と未来の情報をもつデータを入力とできる文章に対して適用可能。


⑤Section5:Seq2Seq
・機械対話　→自然言語処理を活用し、人間の思考等を理解させて対話システムを構築すること。
・機械翻訳　→コンピューターによって「ある言語」を別の言語にオートマチックに翻訳をする方法。
・one-hotベクトル　→(0,1,0,0,0,0)  のように、1つの成分が1で残りの成分が全て0であるようなベクトルのこと。
・embedding表現　→文や単語、文字など自然言語の構成要素に対して、何らかの空間におけるベクトルを与えること。


⑥Section6:Word2vec
・単語を固定長のベクトルで表現することを「単語の分散表現」と呼ぶ。
・単語をベクトルで表現することができれば単語の意味を定量的に把握することができるため、様々な処理に応用することができる。
　言語が「物理的な実数値」ではなく、人間が恣意的に定義した「記号」であり、そのままの形では深層学習で処理できないという課題を解決するために用いられる。
　→Word2Vecは単語の分散表現の獲得を目指した手法。


⑦Section7:Attention Mechanism
・「入力と出力のどの単語が関連しているのか」の関連度を学習する仕組み
　→言い換えると、入力情報全体ではなく、その一部のみを特にフォーカスしたベクトルをデコーダーで使用する仕組みのこと。
　→デコードの特定のタイミングにだけ必要になる入力情報を精度よく出力に反映させることができるようになる。
・Attention Mechanismを導入することによって、翻訳前の文章が長文であっても精度よく翻訳処理が行えるようになった。


