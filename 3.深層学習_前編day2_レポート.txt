深層学習_前編_day2

【要点のまとめ】

①Section1:勾配消失問題
・誤差逆伝播法が下位層に進んでいくに連れて、勾配がどんどん緩やかになっていく。
　　そのため、勾配降下法による更新では下位層のパラメータはほとんど変わらず、訓練は最適値に収束しなくなる。
・中間層が増えると、誤差逆伝播(の微分の連鎖律)で使用する微分成分が大量に存在することになる。
　→微分値が0～1の値をとる場合(例:シグモイド関数)、連鎖律の掛け算は微分成分が増えるほど小さな値の出力となる。
・勾配消失の解決方法
　→活性化関数の選択、重みの初期値設定、バッチ正規化
・活性化関数の選択→ReLU関数　x>0:f(x)=x, x<=0:f(x)=0
・重みの初期値設定→通常は初期値に(標準正規分布に従う)乱数を設定。
　　　　　　　　　→勾配消失問題に強い方法として、Xavierがある。[S字カーブの活性化関数(シグモイド関数など)に有効]
　　　　　　　　　　(前の層のノード数の平方根で除算した値を用いる。)

　　　　　　　　　→別の方法として、Heがある。[S字カーブじゃない活性化関数(ReLU関数など)に有効]
　　　　　　　　　　(前の層のノード数の平方根で除算した値に対し、root(2)を掛け合わせた値を用いる。)
・バッチ正規化→ミニバッチ単位で入力値のデータの偏りを抑制する手法。


②Section2:学習率最適化手法
・勾配降下法:w = w -ε▽E　→ε:学習率
・学習率の決め方とは
　→初期の学習率設定方法の指針
　　・初期の学習率を大きく設定し、徐々に学習率を小さくしていく。
　　・パラメータ毎に学習率を可変させる。
　→学習率最適化手法を利用して学習率を最適化。
・学習率最適化手法(4つ)
　→モメンタム、AdaGrad、RMSProp、Adam
・モメンタム　→誤差をパラメータで微分したものと学習率の積を減算した後、
　　　　　　　　現在の重みに前回の重みを減算した値と慣性(μ:ハイパーパラメータ)の積を加算する。
・AdaGrad　→誤差をパラメータで微分したものと再定義した学習率の積を減算する。
・RMSProp　→誤差をパラメータで微分したものと再定義した学習率の積を減算する。
　　　　　　 AdaGradの改良版。
・Adam　→下記をそれぞれ孕んだ最適化アルゴリズム。
　　　　　・モメンタムの、過去の勾配の指数関数的減衰平均
　　　　　・RMSPropの、過去の勾配の2乗の指数関数的減衰平均


③Section3:過学習
・テスト誤差と訓練誤差とで学習曲線が乖離すること。→特定の訓練サンプルに対して、特化して学習されてしまう。
・過学習を抑制する方法→正則化
　ニューラルネットワークの自由度が高いと過学習になりやすいので、正則化により自由度(層数、ノード数、パラメータの値etc...)を制約する。
・正則化手法について
　→・L1正則化、L2正則化
　　・ドロップアウト
・L1正則化、L2正則化について
　過学習の原因:重みが大きい値をとることで、過学習が発生することがある。
　→誤差に対して、正則化項を加算することで、重みを抑制する。
・ドロップアウトとは
　過学習の原因:ノードの数が多い。
　→学習の度、ランダムにノードを削除して学習させること。
　→データ量を変化させずに、異なるモデルを学習させていると解釈できる。


④Section4:畳み込みニューラルネットワークの概念
・画像の識別、処理によく使われるニューラルネットワーク。画像以外でも、次元間でつながりのあるデータを扱える。
・CNNの構造図(例)
　入力層(入力画像)→畳み込み層→畳み込み層→プーリング層→畳み込み層→畳み込み層→プーリング層→全結合層→出力層(出力画像)
・全結合層→今までのニューラルネットワークの処理のこと。
・畳み込み層→計算過程:入力値*フィルター ⇒ 出力値＋バイアス ⇒ 活性化関数 ⇒ 出力値
　(フィルターにより、周りのデータ[画像の場合、ピクセル]とのつながりを保持できる。)
　・パディング→フィルターの大きさにより、出力画像のサイズが変化する(小さくなる)。
　　　　　　　　問題:何度も繰り返すと、画像が小さくなりデータが減っていく。
　　　　　　　→フィルターを通す前に、入力画像の周りのピクセルを増やす。
　　　　　　　　(増やしたピクセルにはどんなデータを入れるか→0 or 近いピクセルのデータ etc..)
　・ストライド→フィルターを動かす大きさを示す。
　　　　　　　　ストライド:1のとき、1個ずつ隣に移動。
　　　　　　　　ストライド:2のとき、2個隣に移動。
・プーリング層→対象領域のMax値または平均値を取得。
　　　　　　　　(Max Pooling、Ave Pooling)


⑤Section5:最新のCNN
・AlexNet
　→モデルの構造:5層の畳み込み層およびプーリング層など、それに続く3層の全結合層から構成される。
　→過学習を防ぐために、全結合層の出力にドロップアウトを使用している。
・畳み込み層→全結合層への移行
　⇒Flattenの処理を利用
　　3次元:(13×13×256) → 1次元:(43264)に変換

　　他:参考　・Global Max Pooling
　　　　　　 ・Global Ave Pooling


