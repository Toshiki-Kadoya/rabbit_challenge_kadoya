深層学習_前編_day1

【確認テスト等の考察結果】

①Section1:入力層～中間層
・ディープラーニングは何をしようとしているか？
　→明示的なプログラムの代わりに、多数の中間層をもつニューラルネットワークを用いて、
　　入力値から目的とする出力値に変換する数学モデルを構築すること。
　　最適化の最終目的:w(重み)、b(バイアス)

　⇒例えば、データにフィッティングする線形or非線形モデルを作るためには、各変数の傾き(重み)及び切片の情報が必要。
　　深層学習では、導出過程(近似の方法)こそ異なるものの、やっていることの方向性は同じ。

・入力層には、使用する説明変数(各特徴量)を代入。
・中間層は、入力された値をその層の重み・バイアスによりコンバートする役割。
　また、学習により都度、重み・バイアスに更新がかかる。


②Section2:活性化関数
・線形、非線形の違いとは
　線形→加法性（関数同士の足し算は両者の和の結果となること。）、斉次性（f(kx) = k*f(x)：任意のベクトルx、任意のスカラーk）を満たすもの。
　　例)x,2x,Ax
　非線形→加法性、斉次性を満たさないもの。
　　例)x**2,sinx,exp(x),logx
　　　→斉次性に難があるものが多い印象。


③Section3:出力層
・誤差関数：平均二乗誤差について
　なぜ、引き算のままではなく二乗するのか？
　　→引き算を行うだけでは、各ラベルでの誤差で正負両方の値が発生し、全体の誤差を正しくあらわすのに都合が悪い。
　　　2乗してそれぞれのラベルでの誤差を正の値になるようにする。
　　⇒+-の符号が異なるまま総和をとると、誤差情報が消失してしまう。
　　　+-の方向に意味はないため、絶対値をとることに支障はない。絶対値をとる処理は簡単なので、採用されているものと推察する。

　1/2はどういう意味か？
　　→実際にネットワークを学習するときに行う誤差逆伝搬の計算で、誤差関数の微分を用いるが、その際の計算式を簡単にするため。本質的な意味はない。
　　⇒本質的な意味はないといいながら、例えば、二乗することにより純粋な誤差情報が一乗の場合よりも大きく出てしまう。
　　　それを和らげる効果をみているのでは？


④Section4:勾配降下法
・オンライン学習とは何か？
　→学習データが入ってくるたびに都度パラメータを更新し、学習を進めていく方法。
　　一方、バッチ学習では一度にすべての学習データを使ってパラメータ更新を行う。
　⇒学習を行う際に1からモデルを作り直すのではなく、新しいデータによる学習のみで今あるモデルのパラメータを随時更新していく流れ。
　　追加で学習させたいときに、効率が良く、コスト削減できる。


⑤Section5:誤差逆伝播法
・誤差逆伝播法では不要な再帰的処理を避ける事が出来る。
　既に行った計算結果を保持しているソースコードを抽出せよ。
　→dE/dy：delta2 = functions.d_mean_squared_error(d, y)
　→dE/dy*dy/du：delta2 = functions.d_mean_squared_error(d, y)
　→dE/dy*dy/du*dudw：grad['W2'] = np.dot(z1.T, delta2)
　⇒出力層から中間層(2層目)、[中間層(1層目)]とさかのぼって計算。
　　各層の要素(計算結果)を用いる。(微分の連鎖律により、各要素に分解したときに出現。)
　　プログラム上、既に行った計算結果を次の計算に使うことになる。


