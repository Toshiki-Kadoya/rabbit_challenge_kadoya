深層学習_前編_day2

【確認テスト等の考察結果】

①Section1:勾配消失問題
・シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値はいくらか？
　→0.25
　⇒誤差逆伝播法で、活性化関数としてシグモイド関数を利用するとき、
　　誤差の計算にシグモイド関数の微分を用いる。中間層が増えるほどシグモイド関数の微分の
　　使用数も増えるが、最大で0.25までの値しかとらないことから、複数の掛け算で0に近づきやすい。
・重みの初期値に0を設定すると、どのような問題が発生するか？
　→重みを0で初期化すると正しい学習が行えない。
　　すべての重みの値が均一に更新されるため、多数の重みをもつ意味がなくなる。
　⇒重みの個性がなくなり、異なる学習アプローチが取れなくなる。
・一般的に考えられるバッチ正規化の効果を2点挙げよ。
　→中間層の重みの更新が安定化される。学習がスピードアップする。
　　過学習を抑えることができる。バッチ正規化により学習データの分布を正規化するため、学習データの極端なばらつきが抑えられる。


②Section2:学習率最適化手法
・モメンタム、AdaGrad、RMSPropの特徴をそれぞれ簡潔に説明せよ。
　→・モメンタムは前回の重みパラメータの減算分を次の重み更新に適用。
　　　→谷間についてから最も低い位置(最適値)にいくまでの時間が早く、大域的最適解までたどり着ける。
　　・AdaGrad、RMSPropは前回までの誤差の微分値の積算分を重みの更新に適用。
　　　→勾配の緩やかな斜面に対して、最適値に近づける。
　　　　AdaGradは鞍点問題を引き起こす可能性がある。RMSPropはAdaGradの改良版であり、大域的最適解までたどり着ける。


③Section3:過学習
・L1正則化を表しているグラフはどちらか答えよ。(2次元表現のグラフ)
　→右図。L1正則化項はマンハッタン距離を用いるので、角ばった形。
　⇒第1象限の等高線:誤差関数の等高線を示す。原点を中心とした等高線:正則化項の等高線を示す。
　　両者の交わる点が、誤差関数＋正則化項の和の極小解となる。


④Section4:畳み込みニューラルネットワークの概念
・畳み込み演算
　入力画像(4×4)　　フィルター(3×3)    :計算結果　⇒バイアスを加えるて、出力画像とする。
　3 4 4 2            3 1 2               141 190
  0 8 9 2            8 7 5               105 141
  0 4 3 1            5 4 1
  3 3 5 7

  出力画像の計算(1行1列目)
　3×3 + 4×1 + 4×2 + 0×8 + 8×7 + 9×5 + 0×5 + 4×4 + 3×1 = 141

・サイズ6×6の入力画像を、サイズ2×2のフィルタで畳み込んだ時の出力画像のサイズを答えよ。
　なおストライドとパディングは1とする。
　→パディング1より、入力サイズは7×7。ストライド1なので、出力画像のサイズは7×7。

  @@@@@@@@
　@******@     **
  @******@     **
  @******@
  @******@
  @******@
  @******@
  @@@@@@@@

【公式】
　Oh = [(画像の高さ + 2×パディング高さ - フィルター高さ)/ストライド] + 1

　Ow = [(画像の幅 + 2×パディング幅 - フィルター幅)/ストライド] + 1


⑤Section5:最新のCNN
・flattenの処理[Python]
 →flatten:多次元のリストを一次元に平坦化する方法。

　例: a = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
　　　a.flatten()
　　　→[0, 1, 2, 3, 4, 5, 6, 7, 8]

・Global Average Pooling(GAP)→前の層の各チャンネル(各特徴マップ)の出力を平均したものをまとめて出力。
・Global Max Pooling→→前の層の各チャンネル(各特徴マップ)の最大値をまとめて出力。
