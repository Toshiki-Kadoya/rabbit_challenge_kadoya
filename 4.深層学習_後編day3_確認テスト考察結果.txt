深層学習_後編_day3

【確認テスト等の考察結果】

⓪深層学習全体像の復讐
(最新のCNN)
・サイズ5×5の入力画像を、サイズ3×3のフィルタで畳み込んだ時の出力画像のサイズを答えよ。
　なおストライドは2、パディングは1とする。
　→3×3

入力画像　　　フィルタ　　　ストライド2なので、縦横に2つ移動　
(パディング込み)
0000000   　　 000
0111110        000
0111110        000
0111110
0111110
0111110
0000000


①Section1:再帰型ニューラルネットワークの概念
・RNNのネットワークには大きくわけて3つの重みがある。
　1つは入力から現在の中間層を定義する際にかけられる重み、
　1つは中間層から出力を定義する際にかけられる重みである。残り1つの重みについて説明せよ。
　→前の時間の中間層から現在の中間層に至るところの重み(中間層から中間層への重み)
　⇒再帰構造を持たせるために、RNNでは中間層から中間層の時間的なつながりが必要になる。

・連鎖律の原理を使い、dz/dxを求めよ。z=t**2、t=x+y
　→dz/dx = dz/dt * dt/dx = 2t * 1 = 2(x+y)

・下図のy1をx・s0・s1・win・w・woutを用いて数式で表せ。
　※バイアスは任意の文字で定義せよ。
　※また中間層の出力にシグモイド関数g(x)を作用させよ。
　→s1 = win*x + w*s0 + b
    y1 = g(wout*s1 + c)


②Section2:LSTM
・シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいものを選択肢から選べ。
　→0.25
　⇒最大でも0.25のため、シグモイド関数の微分が連なれば、値が小さくなっていく。⇒勾配消失問題につながる。

・以下の文章をLSTMに入力し空欄に当てはまる単語を予測したいとする。
　文中の「とても」という言葉は空欄の予測においてなくなっても影響を及ぼさないと考えられる。
　このような場合、どのゲートが作用すると考えられるか

　「映画おもしろかったね。ところで、とてもお腹が空いたから何か____。」

　→忘却ゲート
　⇒CECに記録された「とても」というデータを削除するために作用。


③Section3:GRU
・LSTMとCECが抱える課題について、それぞれ簡潔に述べよ。
　→LSTMの課題:パラメータが多数存在するため、学習に計算コストを多く要する。
　⇒CEC、入力ゲート、出力ゲート、忘却ゲートといった4つの部品をもつため、その分パラメータ数が増える。

　→CECの課題:勾配が1となるため、学習機能がない。

・LSTMとGRUの違いを簡潔に述べよ。
　→LSTM:入力ゲート、出力ゲート、忘却ゲート、CECを持つ
　　GRU:リセットゲート、更新ゲートを持つ
　　GRUよりLSTMの方が多くのパラメータを持つため、計算コストが高い。
　　GRUの方が、少ない計算量で学習できる。


④Section4:双方向RNN
・以下は双方向RNNの順伝播を行うプログラムである。順方向については、入力から中間層への重みW_f, 
　一ステップ前の中間層出力から中間層への重みをU_f、逆方向に関しては同様にパラメータW_b, U_bを持ち、
　両者の中間層表現を合わせた特徴から出力層への重みはVである。_rnn関数はRNNの順伝播を表し中間層の系列を
　返す関数であるとする。（か）にあてはまるのはどれか
　→（4）np.concatenate([h_f, h_b[::-1]], axis=1)
　⇒構文木の演習問題と類似。concatenateは、データを1つにまとめる。axis=1の時は、列方向に増設する形で統合。


⑤Section5:Seq2Seq
・下記の選択肢から、seq2seqについて説明しているものを選べ。
（1）時刻に関して順方向と逆方向のRNNを構成し、それら2つの中間層表現を特徴量として利用するものである。
（2）RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる。
（3）構文木などの木構造に対して、隣接単語から表現ベクトル（フレーズ）を作るという演算を再帰的に行い（重みは共通）、文全体の表現ベクトルを得るニューラルネットワークである。
（4）RNNの一種であり、単純なRNNにおいて問題となる勾配消失問題をCECとゲートの概念を導入することで解決したものである。

　→(2)
　⇒(1)は、双方向RNN。
　　(3)は、構文木。
　　(4)は、LSTM。

・seq2seqとHRED、HREDとVHREDの違いを簡潔に述べよ。
　→Seq2seqは、一問一答に対して処理ができるある時系列データから別の時系列データを得るネットワーク。
　　HREDは、Seq2seqの機構にそれまでの文脈の意味ベクトルを解釈に加えられることにすることで、文脈の意味をくみ取った文の変換をできるようにしたもの。
　　VHREDは、HRED当たり障りのない回答しか作れないことへの解決策。
　⇒Seq2seqでは、会話の文脈無視で、応答がなされたが、
　　HREDでは、前の単語の流れに即して応答されるため、より人間らしい文章が生成される。
　　HREDは、同じコンテキスト（発話リスト）を与えられても、答えの内容が毎回会話の流れとしては同じものしか出せないが、
　　その課題に対して、VHREDは、VAEの潜在変数の概念を追加することで解決した。

・VAEに関する下記の説明文中の空欄に当てはまる言葉を答えよ。
　自己符号化器の潜在変数に____を導入したもの。

　→確率分布


⑥Section6:Word2vec
・RNNとword2vecの違いを簡潔に述べよ。
　→RNNは、時系列データを処理するのに適したニューラルネットワーク。
　　Word2vecは、単語の分散表現ベクトルを得る手法
　⇒RNNでは、単語のような可変長の文字列をNNに与えることはできない。
　　そのため、固定長形式で単語を表す必要がある。


⑦Section7:Attention Mechanism
・seq2seqとAttentionの違いを簡潔に述べよ。
　→seq2seqは、1つの時系列データから別の時系列データを得るネットワーク。
　　Attention Mechanismは、時系列データの中身それぞれの関連性に重みをつける手法。
　⇒seq2seqの問題は長い文章への対応が難しいため、Attention Mechanismが使われる。


