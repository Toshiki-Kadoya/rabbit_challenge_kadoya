【自己学習】
★第3章：情報理論


■[演習問題]
第4章：情報量
-----
4.1
I = log2(1/p(X)) = -log2(p(X))
I：情報量、p(X)：事象Xの発生する確率

問4.1.1
1枚のコインを1回投げて表が出る確率：1/2
　→I = -log2(1/2) = -(-1) = 1 [bit]

問4.1.2
2枚のコインを1回投げて表が出る確率：(1/2) * (1/2) = 1/4
　→I = -log2(1/4) = -(-2) = 2 [bit]

問4.1.3
n枚のコインを1回投げて1枚の表が出る確率：nC1 * (1/2)**n = n * (1/2)**n
　→I = -log2[n * (1/2)**n] = -log2(n) -log2[(1/2)**n]
　　　= -log2(n) -(-n) = -log2(n) + n [bit]


第6章：対数と乗算除算の関係
-----
6.1
X = ABのとき
　→log(X) = log(AB) = log(A) + log(B)

-----
6.2
X = A/Bのとき
　→log(X) = log(A/B) = log(A) - log(B)

-----
6.3
X = x1*x2*x3*x4のとき
　→log(X) = log(x1*x2*x3*x4) = log(x1) + log(x2) + log(x3) + log(x4)
           = ∑log(x)


第7章
-----
7.5
ある離散的な事象の確率分布をP(x)としたとき、シャノンエントロピーは、
　-∑P(x)log(P(x))


