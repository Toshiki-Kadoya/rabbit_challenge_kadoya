深層学習_前編_day2

【自己学習】

①Section1:勾配消失問題
・活性化関数の選択:ReLU関数　x>0:f(x)=x, x<=0:f(x)=0
                   →微分結果は0 or 1になる。
　　　　　　　　　 →価値のない重みは0となり、使われなくなる。
　他の活性化関数で勾配消失問題に役に立つものもある。→Leaky ReLU関数など
・ミニバッチの大きさはどう設定するのか？
　→学習に用いる機材スペック(CPU、GPU:1～64枚、TPU:1～256枚)により調整。


②Section2:学習率最適化手法
・モメンタムのメリット
　→・局所的最適解にはならず、「大域的最適解」となる。
　　・谷間についてから最も低い位置(最適値)にいくまでの時間が早い。
・AdaGradのメリット
　→・勾配の緩やかな斜面に対して、最適値に近づける。
　課題
　→学習率が徐々に小さくなるので、鞍点問題を引き起こす事があった。
　　≒大域的最適解に近づけない。
・鞍点問題とは
　→学習時、鞍点にはまり抜け出せなくなることにより、学習が進まなくなってしまうこと。
　鞍点・・・多変数実関数の変域の中で、ある方向で見れば極大値だが別の方向で見れば極小値となる点。
・RSMPropのメリット
　→・局所的最適解にはならず、大域的最適解となる。
　　・ハイパーパラメータの調整が必要な場合が少ない。
・Adamのメリット
　→・モメンタムおよびRMSPropのメリットを孕んだアルゴリズムである。


③Section3:過学習
・エポック数→「一つの訓練データを何回繰り返して学習させるか」の数のこと。学習回数。
・L2正則化(リッジ回帰)[p=2のとき]→L2正則化を使用すると、パラメータの大きさが抑制される。
・L1正則化(ラッソ回帰)[p=1のとき]→L1正則化を使用すると、いくつかのパラメータを 0 にすることができる。→特徴選択を行っている。
・ノルムとは→平面あるいは空間における幾何学的ベクトルの「長さ」の概念の一般化。
　pノルム:p1→x+y[マンハッタン距離]、p2→root(x**2 + y**2)[ユーグリッド距離]
・np.sigh()→入力に取った値の符号に応じて、負:-1・0:0・正:1を出力。signは符号関数。


④Section4:畳み込みニューラルネットワークの概念
・LeNet→1998年に、Yann LeCunによって考案された初の畳み込みネットワーク。
　　　　 構造としては、畳み込み層とプーリング層のセットを2回繰り返すのが特徴。
・畳み込み層→役割:データの特徴を際立たせる層。フィルターには、画像を鮮明にするフィルター、ぼかすフィルター、エッジを際立たせるフィルターなどがある。
・プーリング層→役割:画像の詳細情報を消すための層。
　　　　　　　　メリット:対象の画像内のズレやノイズに対処しやすくなる。
　　　　　　　　　　　　 計算量を大幅に減らせる。
・全結合層→役割:全結合層は、プーリング層からの出力をまとめるために置かれる。
　　　　　　　　 構造的にはニューラルネットワークの中間層と同様。


⑤Section5:最新のCNN
・AlexNet→2012年に、ImageNetコンペティションで優勝したトロント大学SuperVisionチームの開発したネットワーク。
　　　　　 畳み込み層5層にプーリング層3層という、LeNetと比較するとかなり深い層構造になっている。
・VGG→2014年に、ImageNetコンペティションで優勝したオックスフォード大学のチームが開発したネットワーク。
　　　 通常、畳み込み層の局所受容野は5×5ニューロン程度で構成されることが多い。
　　　 しかしVGGでは、局所受容野を3×3と小さくする代わりに、畳み込み層を増加させる方法を採用している。


