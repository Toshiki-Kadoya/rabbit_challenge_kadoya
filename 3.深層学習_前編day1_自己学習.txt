深層学習_前編_day1

【自己学習】

①Section1:入力層～中間層
・万能近似定理→深層ニューラルネットワークが、実数空間において任意の有界な連続関数を近似できることを保証するための定理。
・バイアス→自分自身で、入力された数値に幾らか足したり引いたりして、入力値を調節することができる。(バイアスは入力値を一定の範囲に偏らせるために用いる。)
　　　　　　初期値を設定するが、初めは適切な値が分からないため、色々と実験する。
　　　　　　「事前学習」を使うと、重み・バイアスの初期値をどの程度にしておいたら良いかの目星が付けられる。
・事前学習→ニューラルネットワークを学習させる際に設定する重みの初期値を、
　　　　　　事前に別のニューラルネットワークを学習して求め、最適化させること。


②Section2:活性化関数
・勾配消失問題→機械学習手法のひとつであるニューラルネットワークの設計において、勾配が消失することで学習が進まなくなる技術的な問題のこと。
　　　　　　　　一度勾配が0になると、いくら計算を繰り返しても重みの更新が進まない(重みの値が変わらない)。
　　　　　　　　層を増やすと、重みの更新式(の中の勾配項)が活性化関数の掛け算の連続になる。→指数関数的に勾配が減ってしまう。
・スパース化→元のベクトルや行列の性質を残しつつ、値のほとんどを0にすることを指す。計算する際に、どちらかのベクトルがスパースであれば計算が高速化できる。
　　　　　　　ニューラルネットのスパース化といった場合には、通常はネットワークのパラメータのほとんどを0にする事を指す。


③Section3:出力層
【活性化関数】
・恒等写像:　f(u)=u　→回帰
・シグモイド関数:　f(u)=1/1 + exp(-u)　→二値分類
・ソフトマックス関数:　f(i,u)=exp(u)/Σexp(u)　→多クラス分類：ソフトマックス関数により、総和が1となる出力値を出すことができる。

【誤差関数】回帰→二乗誤差、分類→交差エントロピー
・平均二乗誤差:loss = functions.mean_squares_error = 1/2*Σ(y-d)**2
　→正解値と出力値の差を取り、符号の正負に関わらず和を取るため二乗。→差の総和で評価。
・クロスエントロピー誤差(交差エントロピー誤差):loss = cross_entropy_error = -Σd*log(y)
　→dは出力層の正解の出力、yは出力層の出力。
　　正解から離れるほど、誤差が急激に大きくなる性質をもつ。→学習の速度を速めることに貢献。


④Section4:勾配降下法
・学習率:ε→学習の最適化においてどのくらい値を動かすかというパラメータ。
　　　　　　 学習率を大きくしすぎると発散し、小さくしすぎると収束まで遅くなる。
・オンライン学習→学習データが入ってくるたびに都度パラメータを更新し、学習を進めていく方法。
・バッチ学習→一度にすべての学習データを使ってパラメータ更新し、学習を進めていく方法。
・SIMD並列化→SIMD(single instruction, multiple data)。1つの命令を同時に複数のデータに適用する並列化の形態を指す。


⑤Section5:誤差逆伝播法
・再帰的→あるものについて記述する際に、記述しているものそれ自身への参照が、その記述中にあらわれること。
　　　　　微分の計算を例にとると、「dE/dw = E(w+h) - E(w-h)/2h」の式。
　　　　　→Eの変化量を求めるために、E自身を参照する必要がある。
・微分の連鎖律→複数の関数が合成された合成関数を微分するとき、その導関数がそれぞれの導関数の積で与えられるという関係式のこと。
　　　　　　　　f(y,u,w):　df/dw = df/dy*dy/du*du/dw


