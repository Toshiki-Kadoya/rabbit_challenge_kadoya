深層学習_前編_day1

【要点のまとめ】

⓪プロローグ(1～4)
・識別モデルと生成モデル
　識別モデル:データを目的のクラスに分類する。(例:犬や猫の画像のデータを識別)　→画像認識
　　→決定木、ロジスティック回帰、サポートベクターマシン(SVM)、ニューラルネットワーク
　生成モデル:特定のクラスのデータを生成する。(例:犬らしい画像を人工的に生成)　→画像の超解像度化やテキスト生成に活用
　　→隠れマルコフモデル、ベイジアンネットワーク、変分オートエンコーダー(VAE)、敵対的生成ネットワーク(GAN)
・識別器の開発アプローチ
　→生成モデル、識別モデル、識別関数の3つのアプローチがある。
　　(生成も識別に応用できる。)
・生成モデルと識別モデルの違い
　データの分類時、データの分布は分類結果より複雑なことがある。
　→生成モデルは、分布を(細かく)推定する。
　→識別モデルは、単に分類結果を得るために、データがクラスに属する確率を求める。
・識別モデルと識別関数の違い
　識別モデル→確率的:推論・決定のプロセスを経て識別結果を得る。
　識別モデル→決定的:入力データから識別結果を一気に得る。


①Section1:入力層～中間層
・入力層(特徴量)から識別結果を確率的に出力(出力層)。
・w(重み)、b(バイアス)を適切な値とするために学習。
・ディープラーニングは何をしようとしているか？
　→明示的なプログラムの代わりに、多数の中間層をもつニューラルネットワークを用いて、
　　入力値から目的とする出力値に変換する数学モデルを構築すること。
　　★最適化の最終目的:w(重み)、b(バイアス)
・ニューラルネットワークが対象とする問題→回帰、分類
・入力として取り得るデータ:連続する実数、確率、フラグ値


②Section2:活性化関数
・次の層への出力の大きさを決める非線形の関数。
　　中間層用→ReLU関数、シグモイド(ロジスティック)関数、ステップ関数
　　出力層用→ソフトマックス関数、恒等写像、シグモイド関数(ロジスティック関数)
・ステップ関数→閾値を超えたら発火する関数。出力f(x)は0(x<0) or 1(X>0)。パーセプトロンで利用された関数。
・シグモイド関数→0～1を緩やかに変化する関数。状態に対して、信号の強弱を伝えられる。
　　　　　　　　→大きな値では、出力の変化が微小なため、勾配消失問題を引き起こす事がある。
・ReLU関数→最も使われている活性化関数。出力f(x)は0(x<0) or x(X>0)勾配消失問題の回避とスパース化に貢献。
・活性化関数の効果で、一部の出力は弱く、一部は強く伝播される。


③Section3:出力層
・出力層の役割→各クラス(分類結果)ごとの確率を出力。
・訓練データの正解値とNNから出力される値を比べ、モデルを評価。
　→どのくらい合っていたかを表現するのに、誤差関数:Eを利用。
・出力層の活性化関数
　→中間層の活性化関数との違い
　　【値の強弱】
　　　・中間層:閾値の前後で信号の強弱を調整。
　　　・出力層:信号の大きさ(比率)はそのままに変換。
　　【確率出力】
　　　・分類問題の場合、出力層の出力は0～1の範囲に限定し、総和を1とする必要がある。
・出力層用の活性化関数は、取り組む問題の種類により決まっている。
　→・回帰:恒等写像（誤差関数:二乗誤差）
　　・二値分類:シグモイド関数（誤差関数:交差エントロピー）
　　・多クラス分類:ソフトマックス関数（誤差関数:交差エントロピー）


④Section4:勾配降下法
・勾配降下法を利用して、パラメータを最適化する。
　→w = w -ε▽E　　ε:学習率　　→全サンプルの平均誤差を用いる。
・学習率が大きすぎる場合、最小値にいつまでもたどり着けず発散してしまう。
　学習率が小さい場合、収束するまでに時間がかかってしまう。極小値が複数ある場合、最小値にたどり着く前に、途中の極小値で学習が止まってしまう場合がある。
・確率的勾配降下法
　→ランダムに抽出したサンプルの誤差を用い、学習を進める。
　　[メリット]計算コストの削減、局所極小解に収束するリスク軽減、オンライン学習ができる
・ミニバッチ勾配降下法
　→ランダムに分割したデータの集合(ミニバッチ)に属するサンプルの平均誤差を用い、学習を進める。
　　[メリット]確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる。
・深層学習では、ミニバッチ勾配降下法が一般的に利用されている。
・▽E = dE/dwをどう計算するか？
　→【数値微分】プログラムで微小な数値を生成し、疑似的に微分を計算する一般的な手法。
　　　　　　　　dE/dw = E(w+h) - E(w-h)/2h
　　　　　　　　[デメリット]計算量が非常に多くなってしまう。
　→誤差逆伝播法を利用する。


⑤Section5:誤差逆伝播法
・算出された誤差を出力層側から順に微分し、前の層前の層へと伝播。
　最小限の計算で各パラメータでの微分値を解析的に計算する手法。
・計算結果(の誤差)から微分を逆算することで、不要な再帰的計算を避けて微分を算出できる。
・計算時、微分の連鎖律を利用。出力層から中間層へ逆にたどる過程の、各パラメータを使用し、連鎖律微分ができる。

