深層学習_後編_day3

【要点のまとめ】

①Section1:再帰型ニューラルネットワークの概念
1-1.再帰型ニューラルネットワーク(RNN)の概要
・RNNとは、時系列データに対応可能なニューラルネットワーク。
・時系列データとは、時間的順序に追って一定間隔ごとに観察され、しかも相互に統計的依存関係が求められるようなでーたの系列。
　→具体的には、音声データ・テキストデータなど。
・基本的なネットワーク構造は、これまでの深層学習と変わらない。
・時間的なつながりをどのように組み込むか？
　→前回の中間層の出力をループさせる(次の中間層の入力とする)ことを実施。
・RNNでは重みが3か所に存在:1.入力層から中間層の重み、2.中間層から出力層の重み、3.前の中間層からの重み(中間層から中間層の重み)
・順伝播:前の時間の中間層の出力と重みの積を加算
・RNNの特徴:時系列モデルを扱うには、初期の状態と過去の時間t-1の状態を保持し、そこから次の時間でのtを再帰的に求める再帰構造が必要になる。

1-2.BPTT
・BPTT(Backpropagation Through Time)とは、RNNにおける誤差逆伝播の一種。
・更新すべきパラメータ　→重み(W[in]、W[out]、W):3つ、バイアス(b、c):2つ


②Section2:LSTM
・RNNの課題　→時系列を遡れば遡るほど、勾配が消失していくため、長い時系列の学習が困難。
　　　　　　 →勾配消失の解決方法とは、別で、構造自体を変えて解決したものがLSTM。
・CEC　→それまでの入力値及び出力値を記憶するための機能部。
・勾配消失および勾配爆発の解決方法として、勾配が、1であれば解決できる。
　→1にするためにCECを導入。
・入力ゲート:入力されたデータの記憶の仕方をCECに指示。
・出力ゲート:どんな風にCECの記憶した情報を使うかを学習。
　　→入力ゲートと出力ゲート:今回の入力と前回の出力をもとに学習。
・忘却ゲート:CECには、過去の情報が全て保管されているが、過去の情報が要らなくなった場合、削除することはできず、保管され続ける。
　　　　　　　→過去の情報が要らなくなった場合、そのタイミングで情報を忘却する機能が必要。
　　　　　　　→忘却ゲートを利用。
・覗き穴結合　→CECの保存されている過去の情報を、任意のタイミングで他のノードに伝播させたり、あるいは任意のタイミングで忘却させたい。
　　　　　　　→覗き穴結合を利用。
　　　　　　　→覗き穴結合とは、CEC自身の値に、重み行列を介して伝播可能にした構造。


③Section3:GRU
・LSTMの課題:LSTMでは、パラメータ数が多く、計算負荷が高くなる問題があった。
・GRUとは　→従来のLSTMでは、パラメータが多数存在していたため、計算負荷が大きかった。
　　　　　　 しかし、GRUでは、そのパラメータを大幅に削減し、精度は同等またはそれ以上が望める様になった構造。
・リセットゲート、更新ゲートを持つ。
　　リセットゲート　→過去の情報をどれだけ忘れるかを決定する。
　　更新ゲート　→過去の情報のどれだけを将来に渡す必要があるかを判断する。


④Section4:双方向RNN
・双方向RNN　→過去の情報だけでなく、未来の情報を加味することで、精度を向上させるためのモデル。
　　　　　　　 (実用例:文章の推敲、機械翻訳など)
・文章の場合は、過去～未来の情報すべてを含んでいるので、未来の情報を学習で活用することができる。
・ネットワーク構造　→未来の情報が遡って伝わってくるようになっている。


⑤Section5:Seq2Seq
・Seq2seqとは、Encoder-Decoderモデルの一種を指す。
・2つのNWで構成されるモデル。機械対話や機械翻訳で利用される。
　→1つ目のNW(エンコーダ)で文の意味を抽出。それをもとに2つ目のNW(デコーダ)で別の表現を生成。
・時系列のデータ(Seq)を入力にとり、時系列データ(Seq)を出力する。
・Encoder RNN　→ユーザーがインプットしたテキストデータを、単語等のトークンに区切って渡す構造。
・自然言語のベクトル化(①自然言語を数字[ID]で表す。→②one-hotベクトル表現に変換。→③embedding表現に変換。)
　→embedding表現がEncoder RNNの入力値。
・Decoder RNN　→システムがアウトプットデータを、単語等のトークンごとに生成する構造。(③→②→①の順番でデータ変換。)
・Seq2seqの課題:一問一答しかできない。→問に対して文脈も何もなく、ただ応答が行われる続ける。
・HREDとは　→過去n-1個の発話から次の発話を生成する。　→Seq2Seq+ Context RNN
　　　　　　→Seq2seqでは、会話の文脈無視で、応答がなされたが、
　　　　　　　HREDでは、前の単語の流れに即して応答されるため、より人間らしい文章が生成される。
　　　　　　・Context RNN　→Encoderのまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する構造。
　　・HREDの課題:HRED は確率的な多様性が字面にしかなく、会話の「流れ」のような多様性が無い。
　　　　　　　→同じコンテキスト（発話リスト）を与えられても、答えの内容が毎回会話の流れとしては同じものしか出せない。
・VHREDとは　→HREDに、VAEの潜在変数の概念を追加したもの。
　　　　　　　 HREDの課題を、VAEの潜在変数の概念を追加することで解決した構造。
　　・オートエンコーダとは　→教師なし学習の一つ。
　　・オートエンコーダ具体例:MNISTの場合、28x28の数字の画像を入れて、同じ画像を出力するニューラルネットワーク。
　　・オートエンコーダの構造　→入力データから潜在変数zに変換するニューラルネットワークをEncoder、逆に潜在変数zをインプットとして元画像を復元するニューラルネットワークをDecoder。
　　　メリット:次元削減が行える。
　　・VAE　→通常のオートエンコーダーの場合、何かしら潜在変数zにデータを押し込めているものの、その構造がどのような状態かわからない。
　　　　　 →VAEはこの潜在変数zに確率分布z∼N(0,1)を仮定したもの。
　　　　　 →VAEは、データを潜在変数zの確率分布という構造に押し込めることを可能にする。


⑥Section6:Word2vec
・単語の文字列をベクトル表現にする方法。
・embedding表現を得る手法の一つ。
・①自然言語からボキャブラリを作成→②one-hotベクトル表現に変換→③ボキャブラリ×任意の単語ベクトル次元で重み行列が誕生。
　⇒one-hotベクトルをそのまま使うと入力データが増えてしまうので、変換している。
　　メリット:大規模データの分散表現の学習を、現実的な計算速度とメモリ量で実現可能にした。


⑦Section7:Attention Mechanism
・seq2seqの課題:seq2seqの問題は長い文章への対応が難しい。seq2seqでは、2単語でも、100単語でも、(エンコーダの最終的なアウトプットである)固定次元ベクトルの中に入力しなければならない。
　→解決策:Attention Mechanism　文章の中で重要な単語を見分ける。「入力と出力のどの単語が関連しているのか」の関連度を学習する仕組み。
　→具体的には、
　　・各時系列データのエンコーダでの途中過程を含む処理結果をまとめた情報を、デコーダへ渡す。
　　・上記情報から必要情報を抽出し、デコーダの処理に活用。
・近年精度の高いモデルでは、ほとんどでAttention Mechanismが使用されている。


